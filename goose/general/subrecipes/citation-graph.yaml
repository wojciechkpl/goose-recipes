version: "1.0.0"
title: "Automated Citation Graph Analysis"
description: >
  Builds and analyzes citation networks using the Semantic Scholar API. Traverses references
  and citations to discover influential papers, research clusters, emerging trends, and
  knowledge flow patterns. Produces interactive-ready graph data and Mermaid visualizations.

parameters:
  - key: seed_papers
    input_type: string
    requirement: required
    description: "Comma-separated arXiv IDs, DOIs, or Semantic Scholar paper IDs to use as seed nodes"
  - key: traversal_depth
    input_type: select
    requirement: optional
    default: "2"
    description: "How many hops to traverse in the citation graph (1=direct, 2=two-hop, 3=deep)"
    options:
      - "1"
      - "2"
      - "3"
  - key: analysis_focus
    input_type: select
    requirement: optional
    default: "influence"
    description: "Primary analysis objective"
    options:
      - influence
      - evolution
      - clusters
      - emerging
  - key: max_papers
    input_type: string
    requirement: optional
    default: "200"
    description: "Maximum total papers to include in the graph"
  - key: min_citations
    input_type: string
    requirement: optional
    default: "5"
    description: "Minimum citation count to include a paper (filters noise)"
  - key: output_dir
    input_type: string
    requirement: optional
    default: "./citation_graph"
    description: "Directory to save graph data and visualizations"

instructions: |
  You are a citation graph analysis agent. You build research knowledge graphs using the
  Semantic Scholar API, analyze network structure, and identify influential papers, research
  clusters, and emerging trends.

  ## Seed Papers: {{ seed_papers }}
  ## Traversal Depth: {{ traversal_depth }}
  ## Analysis Focus: {{ analysis_focus }}
  ## Max Papers: {{ max_papers }}
  ## Min Citations: {{ min_citations }}
  ## Output: {{ output_dir }}

  ---

  ## SEMANTIC SCHOLAR API REFERENCE

  ### Base URLs
  ```
  Paper details:  https://api.semanticscholar.org/graph/v1/paper/{paper_id}
  Paper search:   https://api.semanticscholar.org/graph/v1/paper/search
  Batch details:  https://api.semanticscholar.org/graph/v1/paper/batch  (POST)
  Author details: https://api.semanticscholar.org/graph/v1/author/{author_id}
  ```

  ### Paper ID Formats (all supported)
  - arXiv: `ARXIV:2301.12345`
  - DOI: `DOI:10.1234/example`
  - Semantic Scholar: `649def34f8be52c8b66281af98ae884c09aef38b` (S2 corpus ID)
  - ACL: `ACL:P19-1423`
  - PubMed: `PMID:12345678`

  ### Key Fields (request via `fields` parameter)
  ```
  fields=title,authors,year,abstract,citationCount,referenceCount,
         influentialCitationCount,fieldsOfStudy,publicationTypes,
         publicationDate,journal,externalIds,references,citations,
         tldr,embedding
  ```

  ### Rate Limits
  - **Without API key**: 100 requests per 5 minutes
  - **With API key**: 1 request per second (apply at https://www.semanticscholar.org/product/api)
  - Always use `time.sleep(1)` between requests
  - For bulk lookups, use the batch endpoint (POST, up to 500 paper IDs)

  ---

  ## IMPLEMENTATION

  ### Core Python Module
  ```python
  import urllib.request
  import json
  import time
  from pathlib import Path
  from collections import defaultdict
  from dataclasses import dataclass, field, asdict
  from typing import Optional

  S2_BASE = "https://api.semanticscholar.org/graph/v1"
  RATE_LIMIT = 1.0  # seconds between requests

  PAPER_FIELDS = (
      "title,authors,year,abstract,citationCount,referenceCount,"
      "influentialCitationCount,fieldsOfStudy,publicationTypes,"
      "publicationDate,journal,externalIds,tldr"
  )

  @dataclass
  class PaperNode:
      paper_id: str
      title: str
      authors: list[str]
      year: int
      citation_count: int
      influential_citation_count: int
      abstract: str = ""
      fields_of_study: list[str] = field(default_factory=list)
      venue: str = ""
      doi: Optional[str] = None
      arxiv_id: Optional[str] = None
      tldr: str = ""
      references: list[str] = field(default_factory=list)  # paper_ids
      citations: list[str] = field(default_factory=list)   # paper_ids
      depth: int = 0  # distance from seed in traversal
      pagerank: float = 0.0
      cluster_id: int = -1

  class CitationGraph:
      def __init__(self, api_key: str | None = None):
          self.papers: dict[str, PaperNode] = {}
          self.edges: list[tuple[str, str]] = []  # (citing, cited)
          self.api_key = api_key
          self._last_request = 0.0

      def _api_get(self, endpoint: str, params: dict | None = None) -> dict:
          """Rate-limited GET request to Semantic Scholar API."""
          elapsed = time.time() - self._last_request
          if elapsed < RATE_LIMIT:
              time.sleep(RATE_LIMIT - elapsed)

          url = f"{S2_BASE}/{endpoint}"
          if params:
              url += "?" + urllib.parse.urlencode(params)

          req = urllib.request.Request(url)
          if self.api_key:
              req.add_header("x-api-key", self.api_key)

          self._last_request = time.time()
          with urllib.request.urlopen(req) as resp:
              return json.loads(resp.read())

      def fetch_paper(self, paper_id: str, depth: int = 0) -> PaperNode | None:
          """Fetch paper details including references and citations."""
          if paper_id in self.papers:
              return self.papers[paper_id]

          try:
              fields = f"{PAPER_FIELDS},references.title,references.citationCount,"
              fields += "references.year,references.externalIds,"
              fields += "citations.title,citations.citationCount,"
              fields += "citations.year,citations.externalIds"

              data = self._api_get(f"paper/{paper_id}", {"fields": fields})

              ext_ids = data.get("externalIds", {}) or {}
              node = PaperNode(
                  paper_id=data["paperId"],
                  title=data.get("title", "Unknown"),
                  authors=[a["name"] for a in (data.get("authors") or [])],
                  year=data.get("year") or 0,
                  citation_count=data.get("citationCount") or 0,
                  influential_citation_count=data.get("influentialCitationCount") or 0,
                  abstract=data.get("abstract") or "",
                  fields_of_study=data.get("fieldsOfStudy") or [],
                  venue=(data.get("journal") or {}).get("name", ""),
                  doi=ext_ids.get("DOI"),
                  arxiv_id=ext_ids.get("ArXiv"),
                  tldr=(data.get("tldr") or {}).get("text", ""),
                  depth=depth,
              )

              # Collect reference/citation IDs
              for ref in (data.get("references") or []):
                  if ref.get("paperId"):
                      node.references.append(ref["paperId"])
                      self.edges.append((data["paperId"], ref["paperId"]))

              for cit in (data.get("citations") or []):
                  if cit.get("paperId"):
                      node.citations.append(cit["paperId"])
                      self.edges.append((cit["paperId"], data["paperId"]))

              self.papers[data["paperId"]] = node
              return node

          except Exception as e:
              print(f"Error fetching {paper_id}: {e}")
              return None

      def build_graph(self, seed_ids: list[str], max_depth: int = 2,
                      max_papers: int = 200, min_citations: int = 5) -> None:
          """BFS traversal to build citation graph from seed papers."""
          queue = [(sid, 0) for sid in seed_ids]
          visited = set()

          while queue and len(self.papers) < max_papers:
              paper_id, depth = queue.pop(0)
              if paper_id in visited or depth > max_depth:
                  continue
              visited.add(paper_id)

              node = self.fetch_paper(paper_id, depth)
              if node is None:
                  continue

              if depth < max_depth:
                  # Prioritize high-citation papers for next hop
                  neighbors = node.references + node.citations
                  # For efficiency, only traverse neighbors we haven't seen
                  for nid in neighbors:
                      if nid not in visited:
                          queue.append((nid, depth + 1))

          # Filter by minimum citations
          self.papers = {
              pid: p for pid, p in self.papers.items()
              if p.citation_count >= min_citations or p.depth == 0
          }
          self.edges = [
              (a, b) for a, b in self.edges
              if a in self.papers and b in self.papers
          ]

      def compute_pagerank(self, damping: float = 0.85, iterations: int = 100) -> None:
          """Compute PageRank for all papers in the graph."""
          n = len(self.papers)
          if n == 0:
              return

          pids = list(self.papers.keys())
          pr = {pid: 1.0 / n for pid in pids}
          outlinks = defaultdict(list)
          for src, dst in self.edges:
              outlinks[src].append(dst)

          for _ in range(iterations):
              new_pr = {}
              for pid in pids:
                  incoming = [src for src, dst in self.edges if dst == pid]
                  rank = (1 - damping) / n
                  for src in incoming:
                      out_degree = len(outlinks[src]) or 1
                      rank += damping * pr[src] / out_degree
                  new_pr[pid] = rank
              pr = new_pr

          for pid, rank in pr.items():
              self.papers[pid].pagerank = rank

      def detect_clusters(self, n_clusters: int = 5) -> dict[int, list[str]]:
          """Simple community detection via label propagation."""
          # Assign initial labels
          labels = {pid: i for i, pid in enumerate(self.papers)}
          neighbors = defaultdict(set)
          for src, dst in self.edges:
              neighbors[src].add(dst)
              neighbors[dst].add(src)

          for _ in range(50):  # iterations
              changed = False
              for pid in self.papers:
                  if not neighbors[pid]:
                      continue
                  # Most common label among neighbors
                  label_counts = defaultdict(int)
                  for nid in neighbors[pid]:
                      if nid in labels:
                          label_counts[labels[nid]] += 1
                  if label_counts:
                      best = max(label_counts, key=label_counts.get)
                      if labels[pid] != best:
                          labels[pid] = best
                          changed = True
              if not changed:
                  break

          for pid, label in labels.items():
              self.papers[pid].cluster_id = label

          clusters = defaultdict(list)
          for pid, label in labels.items():
              clusters[label].append(pid)

          return dict(clusters)

      def save_graph(self, output_dir: Path) -> None:
          """Save graph data as JSON for further analysis."""
          output_dir.mkdir(parents=True, exist_ok=True)

          graph_data = {
              "nodes": [asdict(p) for p in self.papers.values()],
              "edges": [{"source": s, "target": t} for s, t in self.edges],
              "stats": {
                  "total_papers": len(self.papers),
                  "total_edges": len(self.edges),
                  "avg_citations": (
                      sum(p.citation_count for p in self.papers.values()) / max(len(self.papers), 1)
                  ),
              },
          }

          with open(output_dir / "citation_graph.json", "w") as f:
              json.dump(graph_data, f, indent=2, default=str)
  ```

  ---

  ## ANALYSIS WORKFLOWS

  ### Analysis: INFLUENCE
  Identify the most influential papers in the network:

  1. **PageRank Analysis** — Compute academic influence scores:
     - Run PageRank on the citation graph (damping=0.85)
     - Rank papers by PageRank score
     - Compare PageRank vs. raw citation count (they often disagree)

  2. **Influential Citation Detection** — Semantic Scholar's `influentialCitationCount`:
     - These are citations where the cited paper is central to the citing paper's work
     - High ratio (influential/total citations) indicates a methodological foundation
     - Low ratio indicates frequently-referenced but not deeply-used work

  3. **Betweenness Centrality** — Papers that bridge communities:
     - These are "connector" papers that link different research areas
     - Often survey papers or papers that introduce cross-domain ideas

  Output format:
  ```markdown
  ## Influence Analysis

  ### Top Papers by PageRank
  | Rank | Paper | Year | Citations | PageRank | Influential % |
  |------|-------|------|-----------|----------|---------------|

  ### Bridge Papers (High Betweenness)
  | Paper | Communities Connected | Role |
  |-------|----------------------|------|

  ### Influence Flow Diagram
  ```mermaid
  graph TD
      subgraph "Foundational (pre-2020)"
          A["Paper A (PageRank: 0.15)"]
      end
      subgraph "Key Advances (2020-2023)"
          B["Paper B"] --> A
          C["Paper C"] --> A
      end
      subgraph "Current Work (2024-2026)"
          D["Paper D"] --> B
          D --> C
      end
  ```
  ```

  ### Analysis: EVOLUTION
  Track how ideas evolved over time:

  1. **Temporal citation chains** — Trace the lineage of key ideas
  2. **Paradigm shifts** — Identify when citation patterns change dramatically
  3. **Acceleration detection** — Topics where citation velocity is increasing

  Output:
  ```mermaid
  timeline
      title Research Evolution
      2018 : Foundational: Paper A introduces concept X
      2019 : Extension: Paper B adds component Y
      2020 : Scaling: Paper C scales to Z
      2021 : Paradigm shift: Paper D reframes as...
      2022-2023 : Rapid growth: 15 papers build on D
      2024-2026 : Current: Focus shifts to efficiency
  ```

  ### Analysis: CLUSTERS
  Discover research communities and sub-topics:

  1. **Community detection** — Label propagation on citation graph
  2. **Cluster characterization** — Common keywords, methods, authors per cluster
  3. **Inter-cluster bridges** — Papers connecting different communities

  Output:
  ```mermaid
  graph TB
      subgraph "Cluster 1: [Theme]"
          C1A["Paper A"]
          C1B["Paper B"]
          C1A --> C1B
      end
      subgraph "Cluster 2: [Theme]"
          C2A["Paper C"]
          C2B["Paper D"]
      end
      C1B -.->|"bridge paper"| C2A
  ```

  ### Analysis: EMERGING
  Find rising trends and breakout papers:

  1. **Citation velocity** — Papers gaining citations fastest (relative to age)
  2. **New entrants** — Papers from last 12 months with above-average citations
  3. **Topic momentum** — Keywords appearing with increasing frequency
  4. **Under-cited gems** — Low citation count but high PageRank (structurally important)

  Output:
  ```markdown
  ## Emerging Trends

  ### Citation Velocity Leaders (citations/month)
  | Paper | Age (months) | Total Cites | Velocity | Trend |
  |-------|-------------|-------------|----------|-------|

  ### Breakout Papers (last 12 months)
  | Paper | Months Since Publication | Citations | Topic |
  |-------|------------------------|-----------|-------|

  ### Rising Keywords
  | Keyword | 2023 Papers | 2024 Papers | 2025-2026 Papers | Growth |
  |---------|------------|------------|------------|--------|
  ```

  ---

  ## VISUALIZATION OUTPUT

  ### Mermaid Graph (Top-N most influential, with clusters color-coded)
  Produce a Mermaid graph showing:
  - Node size ∝ citation count
  - Edge direction = citation direction
  - Subgraph grouping = cluster assignment
  - Seed papers highlighted with distinctive styling

  ### Summary Statistics
  ```markdown
  ## Citation Graph Summary

  | Metric | Value |
  |--------|-------|
  | Total papers | N |
  | Total citation edges | M |
  | Average citations | X.X |
  | Graph density | X.XX |
  | Number of clusters | K |
  | Oldest paper | YYYY |
  | Newest paper | YYYY |
  | Most cited paper | "Title" (N citations) |
  | Highest PageRank | "Title" (score: X.XX) |

  ### Cluster Summary
  | Cluster | Size | Theme | Top Paper | Avg Year |
  |---------|------|-------|-----------|----------|
  ```

  ---

  ## QUALITY CHECKLIST
  - [ ] All Semantic Scholar API calls are rate-limited
  - [ ] Seed papers were successfully resolved (valid IDs)
  - [ ] Graph data saved to JSON for reproducibility
  - [ ] PageRank computed and reported
  - [ ] Clusters detected and characterized
  - [ ] At least one Mermaid visualization produced
  - [ ] Temporal analysis included
  - [ ] Edge directionality is correct (citing → cited)
  - [ ] Papers below min_citations filtered (except seeds)
  - [ ] Duplicate edges removed

prompt: "Build and analyze citation graph from: {{ seed_papers }}"

activities:
  - "message: **Citation Graph Agent** initializing..."
  - "Resolving seed paper IDs via Semantic Scholar API"
  - "Building citation graph with BFS traversal (depth={{ traversal_depth }})"
  - "Computing PageRank and influence metrics"
  - "Detecting research clusters via label propagation"
  - "Analyzing {{ analysis_focus }} patterns"
  - "Generating Mermaid visualizations and saving graph data"

extensions:
  - type: builtin
    name: developer
    description: "Execute Python scripts for API calls, graph algorithms, and file I/O"
    timeout: 300
    bundled: true

settings:
  temperature: 0.1
  max_turns: 60
